{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a string\n",
    "string = '''\n",
    "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
    "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
    "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
    "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But what if we want to read a text file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\insofe data\\Rstudio\\lab29_28_1_18__text_mining&natural langauage processing\n"
     ]
    }
   ],
   "source": [
    "#We first check the working directory\n",
    "import os\n",
    "#os.chdir('')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '20170128_Batch34_CSE7124c_TextPreprocessing_Lab01.ipynb', '20170128_Batch34_CSE7124c_TextPreprocessing_Lab01.zip', '20180128_Batch34_CSE7124c_Text_Mining_Lecture01(1).pdf', '20180128_Batch34_CSE7124c_Text_Mining_Lecture01.pdf', '20180128_Batch34_CSE7124c_TfIdfManual_Lab01.xlsx', 'sherlock.txt']\n"
     ]
    }
   ],
   "source": [
    "#we could even verify what files are there in the path as shown:\n",
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading the text file\n",
    "with open('sherlock.txt', 'r') as file_:\n",
    "    string = file_.read()\n",
    "    \n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'do', \"n't\", 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', ',', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', '.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', '.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', ',', 'his', 'arms', 'folded', ',', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', ',', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', ',', 'buried', 'in', 'the', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'he', 'started', ',', 'tapped', 'me', 'on', 'the', 'shoulder', ',', 'and', 'pointed', 'over', 'the', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Tockenizing the words\n",
    "tokens = nltk.word_tokenize(string)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
      "\n",
      "\n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.\n",
      "\n",
      "\n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.\n",
      "\n",
      "\n",
      "To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.\n",
      "\n",
      "\n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.\n",
      "\n",
      "\n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n"
     ]
    }
   ],
   "source": [
    "# Creating sentence tokens and printing the sentences\n",
    "sentences = nltk.sent_tokenize(string)\n",
    "for sent in sentences:\n",
    "    print('\\n')\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice how \"don't\" has been tokenized to 'do' and \"n't\" in word tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowering case\n",
    "###### (Here's a nice use of list comprehensions. Instead of writing a for loop, initiating an empty list and appending lowered words, we can save much space by doing this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', \"n't\", 'train', 'leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', '.', 'perfect', 'day', ',', 'bright', 'sun', 'fleecy', 'clouds', 'heavens', '.', 'trees', 'wayside', 'hedges', 'throwing', 'first', 'green', 'shoots', ',', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', '.', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', '.', 'companion', 'sat', 'front', 'trap', ',', 'arms', 'folded', ',', 'hat', 'pulled', 'eyes', ',', 'chin', 'sunk', 'upon', 'breast', ',', 'buried', 'deepest', 'thought', '.', 'suddenly', ',', 'however', ',', 'started', ',', 'tapped', 'shoulder', ',', 'pointed', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "lower_tokens = []\n",
    "for token in tokens:\n",
    "    lower_tokens.append(token.lower())\n",
    "'''\n",
    "##This is the same as doing the following lines of code\n",
    "lower_tokens = [token.lower() for token in tokens]\n",
    "print(lower_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', \"n't\", 'train', 'leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', '.', 'perfect']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Reading the stopwords from the corpus\n",
    "stop = stopwords.words('english')\n",
    "# Removing the stop words from the text\n",
    "tokens = [token for token in lower_tokens if token not in stop]\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', \"n't\", 'train', 'leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'lovely', 'surrey', 'lane', '.', 'perfect']\n"
     ]
    }
   ],
   "source": [
    "lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni', 'comput', 'comput', 'comput', 'better']\n",
      "['caress', 'fly', 'dy', 'mule', 'denied', 'computer', 'computing', 'compute', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Stemming the text using porter stemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied','computer','computing','compute','better']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(singles)\n",
    "lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "tokensLmtz = [lmtzr.lemmatize(token) for token in plurals]\n",
    "print(tokensLmtz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------RAW TEXT---------------------\n",
      "'When I'M a Duchess,' she said 123 & * ) to herself, (not in a very hopeful tone\n",
      "... though), 'I won't have #4 34 ' ' AB any pepper in m3y kitchen 456 AT ALL. Soup does very\n",
      "... well without--Maybe it's # % ! always pepper that make?s people hot-tempered,?!!^'...\n"
     ]
    }
   ],
   "source": [
    "print('----------------RAW TEXT---------------------')\n",
    "\n",
    "import re\n",
    "raw = \"\"\"'When I'M a Duchess,' she said 123 & * ) to herself, (not in a very hopeful tone\n",
    "... though), 'I won't have #4 34 ' ' AB any pepper in m3y kitchen 456 AT ALL. Soup does very\n",
    "... well without--Maybe it's # % ! always pepper that make?s people hot-tempered,?!!^'...\"\"\"\n",
    "print(raw)\n",
    "output = re.sub(r'\\d+', '', raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing numbers\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-121fa2d9e53c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After removing numbers\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"(\\.|\\!|\\?)-'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnewstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"After removing numbers\")\n",
    "print(output)\n",
    "\n",
    "p = re.compile(\"(\\.|\\!|\\?)-'\")\n",
    "newstr = p.sub(\"\", output)\n",
    "print(\"after removing unwanted characters\")\n",
    "print(newstr)\n",
    "\n",
    "singles = re.findall(r'\\w+', newstr)\n",
    "print(singles)\n",
    "\n",
    "\n",
    "print('----------------replacing specific words---------------------')\n",
    "\n",
    "rexexample = [w for w in singles if re.search('^pep', w)]\n",
    "print(rexexample)\n",
    "\n",
    "print('----------------after replacing---------------------')\n",
    "\n",
    "rexexample1 = raw.replace('pepper',\"testingwords\")\n",
    "print(rexexample1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the above functions into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying the necessary text preprocessing steps on the data\n",
    "def process_text(text):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    sentence_tokens = [nltk.tokenize.word_tokenize(sentence) for sentence in sentences]\n",
    "    tokens = []\n",
    "    for sentence in sentence_tokens:\n",
    "        sent = []\n",
    "        for word in sentence:\n",
    "            if word.lower() not in stop:\n",
    "                sent.append(word.lower())\n",
    "        tokens.append(sent)\n",
    "    ##THE SAME FOR LOOP CAN BE WRITTEN AS FOLLOWS\n",
    "    ##tokens = [[word.lower() for word in sent if word not in stop] for sent in sentence_tokens]\n",
    "    tokens = [[lmtzr.lemmatize(word) for word in sent] for sent in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "\n",
      "[['waterloo', 'fortunate', 'catching', \"n't\", 'train', 'leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'lovely', 'surrey', 'lane', '.'], ['perfect', 'day', ',', 'bright', 'sun', 'fleecy', 'cloud', 'heaven', '.'], ['tree', 'wayside', 'hedge', 'throwing', 'first', 'green', 'shoot', ',', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', '.'], ['least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', '.'], ['companion', 'sat', 'front', 'trap', ',', 'arm', 'folded', ',', 'hat', 'pulled', 'eye', ',', 'chin', 'sunk', 'upon', 'breast', ',', 'buried', 'deepest', 'thought', '.'], ['suddenly', ',', 'however', ',', 'started', ',', 'tapped', 'shoulder', ',', 'pointed', 'meadow', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Calling the function\n",
    "sentenced_tokens = process_text(string)\n",
    "print(string)\n",
    "print(sentenced_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 11),\n",
       " ('.', 6),\n",
       " ('trap', 2),\n",
       " ('upon', 2),\n",
       " ('waterloo', 1),\n",
       " ('fortunate', 1),\n",
       " ('catching', 1),\n",
       " (\"n't\", 1),\n",
       " ('train', 1),\n",
       " ('leatherhead', 1),\n",
       " ('hired', 1),\n",
       " ('station', 1),\n",
       " ('inn', 1),\n",
       " ('drove', 1),\n",
       " ('four', 1),\n",
       " ('five', 1),\n",
       " ('mile', 1),\n",
       " ('lovely', 1),\n",
       " ('surrey', 1),\n",
       " ('lane', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = nltk.FreqDist(tokens) #WE INITIALIZE AN EMPTY FREQUENCY COUNTER\n",
    "word_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 11,\n",
       "          '.': 6,\n",
       "          'air': 1,\n",
       "          'arm': 1,\n",
       "          'breast': 1,\n",
       "          'bright': 1,\n",
       "          'buried': 1,\n",
       "          'catching': 1,\n",
       "          'chin': 1,\n",
       "          'cloud': 1,\n",
       "          'companion': 1,\n",
       "          'contrast': 1,\n",
       "          'day': 1,\n",
       "          'deepest': 1,\n",
       "          'drove': 1,\n",
       "          'earth': 1,\n",
       "          'engaged': 1,\n",
       "          'eye': 1,\n",
       "          'first': 1,\n",
       "          'five': 1,\n",
       "          'fleecy': 1,\n",
       "          'folded': 1,\n",
       "          'fortunate': 1,\n",
       "          'four': 1,\n",
       "          'front': 1,\n",
       "          'full': 1,\n",
       "          'green': 1,\n",
       "          'hat': 1,\n",
       "          'heaven': 1,\n",
       "          'hedge': 1,\n",
       "          'hired': 1,\n",
       "          'however': 1,\n",
       "          'inn': 1,\n",
       "          'lane': 1,\n",
       "          'least': 1,\n",
       "          'leatherhead': 1,\n",
       "          'lovely': 1,\n",
       "          'meadow': 1,\n",
       "          'mile': 1,\n",
       "          'moist': 1,\n",
       "          \"n't\": 1,\n",
       "          'perfect': 1,\n",
       "          'pleasant': 1,\n",
       "          'pointed': 1,\n",
       "          'promise': 1,\n",
       "          'pulled': 1,\n",
       "          'quest': 1,\n",
       "          'sat': 1,\n",
       "          'shoot': 1,\n",
       "          'shoulder': 1,\n",
       "          'sinister': 1,\n",
       "          'smell': 1,\n",
       "          'spring': 1,\n",
       "          'started': 1,\n",
       "          'station': 1,\n",
       "          'strange': 1,\n",
       "          'suddenly': 1,\n",
       "          'sun': 1,\n",
       "          'sunk': 1,\n",
       "          'surrey': 1,\n",
       "          'sweet': 1,\n",
       "          'tapped': 1,\n",
       "          'thought': 1,\n",
       "          'throwing': 1,\n",
       "          'train': 1,\n",
       "          'trap': 2,\n",
       "          'tree': 1,\n",
       "          'upon': 2,\n",
       "          'waterloo': 1,\n",
       "          'wayside': 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('word_freqs.json', 'w') as f:\n",
    "    json.dump(word_count, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'waterloo'), ('waterloo', 'fortunate'), ('fortunate', 'catching'), ('catching', \"n't\"), (\"n't\", 'train'), ('train', 'leatherhead'), ('leatherhead', ','), (',', 'hired'), ('hired', 'trap'), ('trap', 'station'), ('station', 'inn'), ('inn', 'drove'), ('drove', 'four'), ('four', 'five'), ('five', 'mile'), ('mile', 'lovely'), ('lovely', 'surrey'), ('surrey', 'lane'), ('lane', '.'), ('.', 'perfect'), ('perfect', 'day'), ('day', ','), (',', 'bright'), ('bright', 'sun'), ('sun', 'fleecy'), ('fleecy', 'cloud'), ('cloud', 'heaven'), ('heaven', '.'), ('.', 'tree'), ('tree', 'wayside'), ('wayside', 'hedge'), ('hedge', 'throwing'), ('throwing', 'first'), ('first', 'green'), ('green', 'shoot'), ('shoot', ','), (',', 'air'), ('air', 'full'), ('full', 'pleasant'), ('pleasant', 'smell'), ('smell', 'moist'), ('moist', 'earth'), ('earth', '.'), ('.', 'least'), ('least', 'strange'), ('strange', 'contrast'), ('contrast', 'sweet'), ('sweet', 'promise'), ('promise', 'spring'), ('spring', 'sinister'), ('sinister', 'quest'), ('quest', 'upon'), ('upon', 'engaged'), ('engaged', '.'), ('.', 'companion'), ('companion', 'sat'), ('sat', 'front'), ('front', 'trap'), ('trap', ','), (',', 'arm'), ('arm', 'folded'), ('folded', ','), (',', 'hat'), ('hat', 'pulled'), ('pulled', 'eye'), ('eye', ','), (',', 'chin'), ('chin', 'sunk'), ('sunk', 'upon'), ('upon', 'breast'), ('breast', ','), (',', 'buried'), ('buried', 'deepest'), ('deepest', 'thought'), ('thought', '.'), ('.', 'suddenly'), ('suddenly', ','), (',', 'however'), ('however', ','), (',', 'started'), ('started', ','), (',', 'tapped'), ('tapped', 'shoulder'), ('shoulder', ','), (',', 'pointed'), ('pointed', 'meadow'), ('meadow', '.'), ('.', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "bigrams = list(nltk.ngrams(tokens, 2, pad_right = True, right_pad_symbol='</s>', pad_left=True, left_pad_symbol='<s>'))\n",
    "print(bigrams)\n",
    "#bigram=list(nltk.bigrams(tokens))\n",
    "#print(bigram)\n",
    "#bigramword_count = nltk.FreqDist(bigram)\n",
    "#bigramword_count.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple word cloud generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-ace621005600>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import wordcloud\n",
    "wc = wordcloud.WordCloud()\n",
    "img = wc.generate_from_text(string)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.feature_extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features...\n",
      "done in 0.318s.\n"
     ]
    }
   ],
   "source": [
    "# Building tfidf\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Get TF-IDFs.\n",
    "print(\"Extracting tf-idf features...\")\n",
    "#First we initiate an empty tfidf object with specific conditions\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3))#max_df=0.95, min_df=2, stop_words='english' #USE HELP TO SEE WHAT EACH DOES)\n",
    "t0 = time()\n",
    "#Next we give the data for processing\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "# print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21)\t0.173193877657\n",
      "  (0, 327)\t0.105604180658\n",
      "  (0, 333)\t0.173193877657\n",
      "  (0, 339)\t0.0731110861328\n",
      "  (0, 94)\t0.105604180658\n",
      "  (0, 136)\t0.0731110861328\n",
      "  (0, 40)\t0.105604180658\n",
      "  (0, 60)\t0.105604180658\n",
      "  (0, 304)\t0.105604180658\n",
      "  (0, 89)\t0.211208361316\n",
      "  (0, 156)\t0.105604180658\n",
      "  (0, 345)\t0.105604180658\n",
      "  (0, 119)\t0.105604180658\n",
      "  (0, 307)\t0.0865969388284\n",
      "  (0, 258)\t0.0937552526697\n",
      "  (0, 235)\t0.105604180658\n",
      "  (0, 143)\t0.105604180658\n",
      "  (0, 3)\t0.0468776263349\n",
      "  (0, 66)\t0.105604180658\n",
      "  (0, 97)\t0.105604180658\n",
      "  (0, 185)\t0.105604180658\n",
      "  (0, 80)\t0.105604180658\n",
      "  (0, 168)\t0.105604180658\n",
      "  (0, 295)\t0.105604180658\n",
      "  (0, 159)\t0.105604180658\n",
      "  :\t:\n",
      "  (5, 242)\t0.16590780693\n",
      "  (5, 134)\t0.16590780693\n",
      "  (5, 113)\t0.16590780693\n",
      "  (5, 233)\t0.16590780693\n",
      "  (5, 256)\t0.16590780693\n",
      "  (5, 165)\t0.16590780693\n",
      "  (5, 183)\t0.16590780693\n",
      "  (5, 273)\t0.16590780693\n",
      "  (5, 221)\t0.16590780693\n",
      "  (5, 10)\t0.16590780693\n",
      "  (5, 203)\t0.16590780693\n",
      "  (5, 194)\t0.16590780693\n",
      "  (5, 268)\t0.16590780693\n",
      "  (5, 243)\t0.16590780693\n",
      "  (5, 135)\t0.16590780693\n",
      "  (5, 114)\t0.16590780693\n",
      "  (5, 234)\t0.16590780693\n",
      "  (5, 257)\t0.16590780693\n",
      "  (5, 166)\t0.16590780693\n",
      "  (5, 184)\t0.16590780693\n",
      "  (5, 274)\t0.16590780693\n",
      "  (5, 222)\t0.16590780693\n",
      "  (5, 11)\t0.16590780693\n",
      "  (5, 204)\t0.16590780693\n",
      "  (5, 195)\t0.16590780693\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.16660857  0.16660857\n",
      "   0.16660857]\n",
      " [ 0.11806952  0.11806952  0.11806952 ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "dense = tfidf.todense()\n",
    "dense.shape\n",
    "print(dense)\n",
    "import pandas as pd\n",
    "dataframe=pd.DataFrame(dense,columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air</th>\n",
       "      <th>air was</th>\n",
       "      <th>air was full</th>\n",
       "      <th>and</th>\n",
       "      <th>and drove</th>\n",
       "      <th>and drove for</th>\n",
       "      <th>and few</th>\n",
       "      <th>and few fleecy</th>\n",
       "      <th>and his</th>\n",
       "      <th>and his chin</th>\n",
       "      <th>...</th>\n",
       "      <th>were just throwing</th>\n",
       "      <th>where</th>\n",
       "      <th>where we</th>\n",
       "      <th>where we hired</th>\n",
       "      <th>which</th>\n",
       "      <th>which we</th>\n",
       "      <th>which we were</th>\n",
       "      <th>with</th>\n",
       "      <th>with bright</th>\n",
       "      <th>with bright sun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.046878</td>\n",
       "      <td>0.105604</td>\n",
       "      <td>0.105604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.105604</td>\n",
       "      <td>0.105604</td>\n",
       "      <td>0.105604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.073957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166609</td>\n",
       "      <td>0.166609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166609</td>\n",
       "      <td>0.166609</td>\n",
       "      <td>0.166609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11807</td>\n",
       "      <td>0.11807</td>\n",
       "      <td>0.11807</td>\n",
       "      <td>0.104822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.055809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125724</td>\n",
       "      <td>0.125724</td>\n",
       "      <td>0.125724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.043003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096876</td>\n",
       "      <td>0.096876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.073646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 354 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       air  air was  air was full       and  and drove  and drove for  \\\n",
       "0  0.00000  0.00000       0.00000  0.046878   0.105604       0.105604   \n",
       "1  0.00000  0.00000       0.00000  0.073957   0.000000       0.000000   \n",
       "2  0.11807  0.11807       0.11807  0.104822   0.000000       0.000000   \n",
       "3  0.00000  0.00000       0.00000  0.055809   0.000000       0.000000   \n",
       "4  0.00000  0.00000       0.00000  0.043003   0.000000       0.000000   \n",
       "5  0.00000  0.00000       0.00000  0.073646   0.000000       0.000000   \n",
       "\n",
       "    and few  and few fleecy   and his  and his chin       ...         \\\n",
       "0  0.000000        0.000000  0.000000      0.000000       ...          \n",
       "1  0.166609        0.166609  0.000000      0.000000       ...          \n",
       "2  0.000000        0.000000  0.000000      0.000000       ...          \n",
       "3  0.000000        0.000000  0.000000      0.000000       ...          \n",
       "4  0.000000        0.000000  0.096876      0.096876       ...          \n",
       "5  0.000000        0.000000  0.000000      0.000000       ...          \n",
       "\n",
       "   were just throwing     where  where we  where we hired     which  which we  \\\n",
       "0             0.00000  0.105604  0.105604        0.105604  0.000000  0.000000   \n",
       "1             0.00000  0.000000  0.000000        0.000000  0.000000  0.000000   \n",
       "2             0.11807  0.000000  0.000000        0.000000  0.000000  0.000000   \n",
       "3             0.00000  0.000000  0.000000        0.000000  0.125724  0.125724   \n",
       "4             0.00000  0.000000  0.000000        0.000000  0.000000  0.000000   \n",
       "5             0.00000  0.000000  0.000000        0.000000  0.000000  0.000000   \n",
       "\n",
       "   which we were      with  with bright  with bright sun  \n",
       "0       0.000000  0.000000     0.000000         0.000000  \n",
       "1       0.000000  0.166609     0.166609         0.166609  \n",
       "2       0.000000  0.000000     0.000000         0.000000  \n",
       "3       0.125724  0.000000     0.000000         0.000000  \n",
       "4       0.000000  0.000000     0.000000         0.000000  \n",
       "5       0.000000  0.000000     0.000000         0.000000  \n",
       "\n",
       "[6 rows x 354 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
